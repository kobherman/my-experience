
в файле в основном предоставлены гистограммы частоты появления значений тех или иных величин

# tags stats
анализ частоты появления "токенов" в тегах

## word tokenization
анализ идёт по всему количеству тегов, а для подсчёта числа pad токена симулируется ситуация из кода, где при недостатке токенов добавляется pad токен до верхнего предела (восьми)

### Самые частые
100 самых частых слов:
![](stats_imgs/word_tokens_1.png)

100 самых частых слов, не считая pad токен:
![](stats_imgs/word_tokens_2.png)

1,000 самых частых слов (без pad токена):
![](stats_imgs/word_tokens_3.png)



### Нечастые
500,000 самых редких слов:
очень много слов, встречающихся лишь один раз (с нулевым вхождением слов нет, т.к. иначе они бы не попали в словарик)
![](stats_imgs/word_tokens_4.png)


численные количества:
- `n(pad токен)` = 1,616,305
- `n(слова без pad)` = 11,045,455
- `n(самые частые 200к слов, без pad)` = 10,336,244


## tiktoken
применение для токенизации библиотеки `tiktoken`, `"o200k_base"` с добавлением специального токена `<|pad|>` (и `<|endoftext|>`, но он не использовался)
### raw text
исследование всех тегов целиком, как есть (без обрезания и паддинга под общую длину)

#### Частота
100 самых частых токенов:
![](stats_imgs/tiktoken_tokens_raw_1.png)

частота всех (200,000) токенов:
![](stats_imgs/tiktoken_tokens_raw_2.png)

переход от единичного присутствия токена к отсутствию:
![](stats_imgs/tiktoken_tokens_raw_3.png)
судя по графику, если избавиться от лишних токенов, количество параметров, выделяемых на эмбеддинги можно сократить больше, чем вдвое



численные количества:
- `n(' ')` = 1,262,910 (пробел)
- `n(слова без ' ')` = 22,318,850
- `n(самые частые 50к слов, без ' ')` = 22,268,363


### padded text
регулирование по длине 20 токенов

100 самых частых токенов:
![](stats_imgs/tiktoken_tokens_padded_1.png)

частота всех (200,000) токенов:
![](stats_imgs/tiktoken_tokens_padded_2.png)

переход от единичного присутствия токена к отсутствию:
![](stats_imgs/tiktoken_tokens_padded_3.png)


численные количества:
- `n(<|pad|>)` = 4,893,232
- `n(слова без <|pad|>)` = 15,106,768
- `n(самые частые 150к слов, без <|pad|>)` = 15,085,483



---
# images stats
воссоздание эмпирической плотности распределения отдельно для каждого канала для *обучающей выборки* (в Lab, RGB, BGR) и *генерируемых изображений* (в Lab, RGB)
модель работает с изображениями в цветовом пространстве Lab (удобно с точки зрения задачи), а при предобработке изображения загружаются в формате BGR (используется open-cv), поэтому для перепроверки исследуются распределения во многих цветовых пространствах (это дополнительно позволило на практике рассмотреть некоторые тонкости перехода между цветовыми пространствами)

## обучающие данные
### Lab
изначальные 1млн изображений были переведены из единичных `png` изображений в `numpy` батчи, разделённые на 100 файлов
значения каналов получены посредством перевода BGR цветов в Lab с помощь open-cv

![](stats_imgs/imgs_real_lab_L.png)
![](stats_imgs/imgs_real_lab_a.png)
![](stats_imgs/imgs_real_lab_b.png)


### RGB
каналы получены переводом сохранённых Lab каналов в RGB с помощью open-cv

![](stats_imgs/imgs_real_rgb_R.png)
![](stats_imgs/imgs_real_rgb_G.png)
![](stats_imgs/imgs_real_rgb_B.png)


### BGR
каналы получены чтением изначальных фотографий (самый сырой вид данных) с помощью open-cv
в данном случае не будет никакой погрешности, полученной при переходе от одного цветового пространства к другому

![](stats_imgs/imgs_real_bgr_B.png)
![](stats_imgs/imgs_real_bgr_G.png)
![](stats_imgs/imgs_real_bgr_R.png)


## сгенерированные данные
модель генерирует значения примерно в диапазоне `[0,1]`, после чего линейной функцией вручную переводит в привычный Lab диапазон (`L [0,100]; a,b [-128, 127])`, за чем следует конвертация в RGB с помощью open-cv
для удобства обращения по индексам, значения каналов a, b смещены вправо, то есть на графике подписан диапазон `[0, 255]` вместо `[-128, 127]`, но саму картину распределения это не меняет
статистика проводилась с [[#word tokenization|пословной токенизацией]], примерно 15-30 эпох

### Lab
![](stats_imgs/imgs_fake_lab_L.png)
![](stats_imgs/imgs_fake_lab_a.png)
![](stats_imgs/imgs_fake_lab_b.png)

### RGB
![](stats_imgs/imgs_fake_rgb_R.png)
![](stats_imgs/imgs_fake_rgb_G.png)
![](stats_imgs/imgs_fake_rgb_B.png)
